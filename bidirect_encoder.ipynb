{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2603912-1a5f-42fc-99b8-d1cb79c446e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "try:\n",
    "    from bert_score import score as bert_scorer\n",
    "except ImportError:\n",
    "    bert_scorer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d298c83-8b4a-4d3f-b805-9aec004b24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.max_size = max_size\n",
    "        self.word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.counter = Counter()\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        for s in sentences:\n",
    "            self.counter.update(str(s).split())\n",
    "        for word, _ in self.counter.most_common(self.max_size - 4):\n",
    "            if word not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        return np.array([self.word2idx.get(w, 3) for w in str(sentence).split()], dtype=np.int32)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        self.pairs = sorted(zip(src_data, trg_data), key=lambda x: len(x[0]))\n",
    "        \n",
    "    def __len__(self): return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return torch.from_numpy(self.pairs[idx][0]).long(), \\\n",
    "               torch.from_numpy(self.pairs[idx][1]).long()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src, trg = zip(*batch)\n",
    "    return pad_sequence(src, padding_value=0), pad_sequence(trg, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebc0e6f-d388-467e-b806-b28e09eeab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.rnn(embedded, hidden.unsqueeze(0))\n",
    "        return self.fc_out(output.squeeze(0)), hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfebc2b7-41b9-4aa3-b990-ed840e093a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialing the Seq2Seq encoder model\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialing the Seq2Seq encoder model\")\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder, self.decoder, self.device = encoder, decoder, device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        outputs = torch.zeros(trg_len, batch_size, self.decoder.output_dim).to(self.device)\n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else output.argmax(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04de511-6f8d-415a-93d6-5f39deab7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "df = pd.read_csv('clean_data.csv').dropna()\n",
    "\n",
    "def simple_clean(s): return str(s).lower().replace('.', ' .').strip()\n",
    "\n",
    "en_vocab, sp_vocab = Vocab(10000), Vocab(10000)\n",
    "en_vocab.build_vocab(df['english'].apply(simple_clean))\n",
    "sp_vocab.build_vocab(df['spanish'].apply(lambda x: f\"<start> {simple_clean(x)} <end>\"))\n",
    "\n",
    "en_tokens = [en_vocab.tokenize(simple_clean(s)) for s in df['english']]\n",
    "sp_tokens = [sp_vocab.tokenize(f\"<start> {simple_clean(s)} <end>\") for s in df['spanish']]\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(en_tokens, sp_tokens, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(TranslationDataset(x_train, y_train), batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(TranslationDataset(x_test, y_test), batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "enc = Encoder(en_vocab.vocab_size, 128, 256, 0.5)\n",
    "dec = Decoder(sp_vocab.vocab_size, 128, 256, 0.5)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "print(f\"Training on {device}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf858db-def8-4f21-a722-4827cf250616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on database\n",
      "Epoch 1 Loss: 4.3061\n",
      "Epoch 2 Loss: 3.4025\n",
      "Epoch 3 Loss: 3.0381\n",
      "Epoch 4 Loss: 2.8106\n",
      "Epoch 5 Loss: 2.6513\n",
      "Epoch 6 Loss: 2.5317\n",
      "Epoch 7 Loss: 2.4316\n",
      "Epoch 8 Loss: 2.3497\n",
      "Epoch 9 Loss: 2.2773\n",
      "Epoch 10 Loss: 2.2160\n",
      "Epoch 11 Loss: 2.1692\n",
      "Epoch 12 Loss: 2.1264\n",
      "Epoch 13 Loss: 2.0777\n",
      "Epoch 14 Loss: 2.0359\n",
      "Epoch 15 Loss: 2.0018\n",
      "Epoch 16 Loss: 1.9703\n",
      "Epoch 17 Loss: 1.9380\n",
      "Epoch 18 Loss: 1.9110\n",
      "Epoch 19 Loss: 1.8827\n",
      "Epoch 20 Loss: 1.8612\n",
      "Test evaluation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on database\")\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = criterion(output[1:].view(-1, sp_vocab.vocab_size), trg[1:].view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Test evaluation.\")\n",
    "model.eval()\n",
    "raw_preds, raw_refs = [], []\n",
    "with torch.no_grad():\n",
    "    for src, trg in test_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg, teacher_forcing_ratio=0)\n",
    "        preds = output.argmax(2).t().cpu().numpy()\n",
    "        for i, p in enumerate(preds):\n",
    "            p_words = [sp_vocab.idx2word[idx] for idx in p if idx > 3]\n",
    "            t_words = [sp_vocab.idx2word[idx] for idx in trg.t()[i].cpu().numpy() if idx > 3]\n",
    "            raw_preds.append(p_words)\n",
    "            raw_refs.append([t_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba3c6ce-ea7e-41fb-a657-b3649bab0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.2328\n",
      "METEOR Score: 0.5164\n",
      "BERTScore F1: 0.8490\n"
     ]
    }
   ],
   "source": [
    "def get_eval_metrics(model, test_loader, sp_vocab, device):\n",
    "    model.eval()\n",
    "    hypotheses, references = [], []\n",
    "    raw_hyp, raw_ref = [], []       \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in test_loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            preds = output.argmax(2).t().cpu().numpy()\n",
    "            targets = trg.t().cpu().numpy()\n",
    "\n",
    "            for i in range(preds.shape[0]):\n",
    "                p_tokens = []\n",
    "                for idx in preds[i]:\n",
    "                    if idx == 2: break \n",
    "                    if idx > 3: p_tokens.append(sp_vocab.idx2word[idx])\n",
    "                \n",
    "                t_tokens = [sp_vocab.idx2word[idx] for idx in targets[i] if idx > 3]\n",
    "\n",
    "                if len(t_tokens) > 0:\n",
    "                    final_p_tokens = p_tokens if len(p_tokens) > 0 else [\"<empty>\"]\n",
    "                    \n",
    "                    hypotheses.append(final_p_tokens)\n",
    "                    references.append([t_tokens]) \n",
    "                    raw_hyp.append(\" \".join(final_p_tokens))\n",
    "                    raw_ref.append(\" \".join(t_tokens))\n",
    "\n",
    "    if not references:\n",
    "        print(\"No valid reference sentences found.\")\n",
    "        return\n",
    "    #since there was some empty lines within the rows, i needed to clear out the output\n",
    "    chencherry = SmoothingFunction()\n",
    "    avg_bleu = np.mean([sentence_bleu(ref, hyp, smoothing_function=chencherry.method1) \n",
    "                       for ref, hyp in zip(references, hypotheses)])\n",
    "    \n",
    "    avg_meteor = np.mean([meteor_score(ref, hyp) for ref, hyp in zip(references, hypotheses)])\n",
    "\n",
    "    avg_bert = 0.0\n",
    "    if bert_scorer:\n",
    "        P, R, F1 = bert_scorer(raw_hyp, raw_ref, lang=\"es\", verbose=False)\n",
    "        avg_bert = F1.mean().item()\n",
    "\n",
    "    print(f\"BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"METEOR Score: {avg_meteor:.4f}\")\n",
    "    if bert_scorer:\n",
    "        print(f\"BERTScore F1: {avg_bert:.4f}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "get_eval_metrics(model, test_loader, sp_vocab, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
