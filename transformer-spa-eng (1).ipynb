{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4311116,"sourceType":"datasetVersion","datasetId":2539382}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport spacy\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split\nimport random\n\n\n\n\nprint(\"r (Full + Random Split)...\")\n\n\n!python -m spacy download en_core_web_sm\n!python -m spacy download es_core_news_sm\nspacy_eng = spacy.load(\"en_core_web_sm\")\nspacy_esp = spacy.load(\"es_core_news_sm\")\n\n\ntry:\n    \n    df = pd.read_csv('/kaggle/input/eng-spanish/spa-eng/spa.txt', sep='\\t', header=None, names=['English', 'Spanish'], usecols=[0, 1])\nexcept:\n    df = pd.read_csv('/kaggle/input/english-spanish/english_spanish.csv', nrows=None) # nrows=None tümünü okur\n\n\ndf = df.dropna()\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"all data: {len(df)}\")\nprint(f\"train Set: {len(train_df)}\")\nprint(f\"Test Set:   {len(test_df)}\")\n\n\n# 2. TOKENIZER  VOCABULARY\n# ==========================================\ndef tokenize_eng(text):\n    return [tok.text.lower() for tok in spacy_eng.tokenizer(str(text))]\n\ndef tokenize_esp(text):\n    return [tok.text.lower() for tok in spacy_esp.tokenizer(str(text))]\n\nclass Vocabulary:\n    def __init__(self, freq_threshold=2):\n        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self): return len(self.itos)\n\n    def build_vocabulary(self, sentence_list, tokenizer):\n        frequencies = {}\n        idx = 4\n        for sentence in sentence_list:\n            for word in tokenizer(sentence):\n                frequencies[word] = frequencies.get(word, 0) + 1\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self, text, tokenizer):\n        tokenized_text = tokenizer(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\n\nprint(\"Dictionaries are created\")\nvocab_eng = Vocabulary(freq_threshold=2)\nvocab_eng.build_vocabulary(train_df[\"English\"], tokenize_eng)\n\nvocab_esp = Vocabulary(freq_threshold=2)\nvocab_esp.build_vocabulary(train_df[\"Spanish\"], tokenize_esp)\n\nprint(f\"Vocab Eng: {len(vocab_eng)} | Vocab Esp: {len(vocab_esp)}\")\n\n\n# 3. DATASET  DATALOADER\n\nclass EngSpaDataset(Dataset):\n    def __init__(self, df, vocab_eng, vocab_esp):\n        self.df = df\n        self.vocab_eng = vocab_eng\n        self.vocab_esp = vocab_esp\n        \n    def __len__(self): return len(self.df)\n    \n    def __getitem__(self, index):\n       \n        row = self.df.iloc[index]\n        eng_text = row[\"English\"]\n        esp_text = row[\"Spanish\"]\n        \n        eng_indices = [1] + self.vocab_eng.numericalize(eng_text, tokenize_eng) + [2]\n        esp_indices = [1] + self.vocab_esp.numericalize(esp_text, tokenize_esp) + [2]\n        \n        return torch.tensor(eng_indices), torch.tensor(esp_indices)\n\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n    def __call__(self, batch):\n        src = [item[0] for item in batch]\n        tgt = [item[1] for item in batch]\n        src = pad_sequence(src, batch_first=True, padding_value=self.pad_idx)\n        tgt = pad_sequence(tgt, batch_first=True, padding_value=self.pad_idx)\n        return src, tgt\n\n# Loader\nBATCH_SIZE = 64\n\ntrain_dataset = EngSpaDataset(train_df, vocab_eng, vocab_esp)\ntest_dataset = EngSpaDataset(test_df, vocab_eng, vocab_esp)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=MyCollate(pad_idx=0))\n# Test loader shuffle=False \ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=MyCollate(pad_idx=0))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\n# 1. Positional Encoding\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x: [seq_len, batch_size, d_model]\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n# 2.  Transformer Model\nclass TransformerModel(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8, \n                 num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=512, dropout=0.1):\n        super(TransformerModel, self).__init__()\n        \n        self.d_model = d_model\n        \n        #  (Embedding)\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        \n        \n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        \n        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, \n                                          num_encoder_layers=num_encoder_layers, \n                                          num_decoder_layers=num_decoder_layers, \n                                          dim_feedforward=dim_feedforward, \n                                          dropout=dropout)\n        \n        \n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        \n        \n        self.src_mask = None\n        self.tgt_mask = None\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, src, tgt):\n        # src: [batch_size, src_len] -> [src_len, batch_size] \n        src = src.transpose(0, 1)\n        tgt = tgt.transpose(0, 1)\n\n        \n        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(src.device)\n        \n      \n        src_padding_mask = (src == 0).transpose(0, 1) # 0 = <pad>\n        tgt_padding_mask = (tgt == 0).transpose(0, 1)\n\n        # Embedding + Positional Encoding\n        src = self.src_embedding(src) * math.sqrt(self.d_model)\n        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n        \n        src = self.pos_encoder(src)\n        tgt = self.pos_encoder(tgt)\n        \n     \n        out = self.transformer(src, tgt, \n                               tgt_mask=tgt_mask,\n                               src_key_padding_mask=src_padding_mask,\n                               tgt_key_padding_mask=tgt_padding_mask)\n        \n        \n        return self.fc_out(out.transpose(0, 1))\n\nprint(\"The Model Architecture is Ready\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# 2. Hiperparametrs\nINPUT_DIM = len(vocab_eng)\nOUTPUT_DIM = len(vocab_esp)\nD_MODEL = 256\nN_HEAD = 8\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nFEEDFORWARD_DIM = 512\nDROPOUT = 0.1\n\n\nmodel = TransformerModel(INPUT_DIM, OUTPUT_DIM, D_MODEL, N_HEAD, \n                         NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n                         FEEDFORWARD_DIM, DROPOUT).to(device)\n\n# 4. (Xavier Initialization )\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        if 'weight' in name and param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n            \nmodel.apply(init_weights)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.0005)\n\n\nPAD_IDX = vocab_esp.stoi[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\nprint(f\"Model parameter number: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, (src, trg) in enumerate(iterator):\n        src = src.to(device)\n        trg = trg.to(device)\n        \n        optimizer.zero_grad()\n        \n        \n        output = model(src, trg[:, :-1])\n        \n        \n        \n        output_dim = output.shape[-1]\n        \n        output = output.contiguous().view(-1, output_dim)\n        trg = trg[:, 1:].contiguous().view(-1) z\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        \n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        \n        if i % 50 == 0:\n            print(f\"Step: {i}, Loss: {loss.item():.4f}\")\n        \n    return epoch_loss / len(iterator)\n\n\nNUM_EPOCHS = 10\nCLIP = 1\n\nprint(f\"train starting ({NUM_EPOCHS} Epoch)\")\n\nfor epoch in range(NUM_EPOCHS):\n    start_time = time.time()\n    \n    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n    \n    end_time = time.time()\n    mins, secs = divmod(end_time - start_time, 60)\n    \n    print(f'Epoch: {epoch+1:02} | Time: {int(mins)}m {int(secs)}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport nltk\nfrom bert_score import score as bert_score\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\n\n\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\n\n\ndef translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_len=50):\n    model.eval()\n    if isinstance(sentence, str):\n        tokens = [token.text.lower() for token in spacy_eng.tokenizer(sentence)]\n        src_indices = [1] + [src_vocab.stoi.get(token, 3) for token in tokens] + [2]\n        src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n    else:\n        src_tensor = sentence.unsqueeze(0).to(device)\n    \n    tgt_indices = [1]\n    for i in range(max_len):\n        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n        with torch.no_grad():\n            output = model(src_tensor, tgt_tensor)\n        best_guess = output.argmax(2)[:, -1].item()\n        tgt_indices.append(best_guess)\n        if best_guess == 2: break\n            \n    translated_words = [tgt_vocab.itos[idx] for idx in tgt_indices]\n    if \"<sos>\" in translated_words: translated_words.remove(\"<sos>\")\n    if \"<eos>\" in translated_words: translated_words.remove(\"<eos>\")\n    return \" \".join(translated_words)\n\n\nprint(\"The Exam Starts On The ENTIRE Test Set... \")\n\n\ntest_subset = test_df \n\nhypotheses = [] \nreferences = [] \nreferences_for_meteor = [] \n\nprint(f\"all sentences: {len(test_subset)}\")\n\n\nfor i, row in tqdm(test_subset.iterrows(), total=len(test_subset)):\n    src = row[\"English\"]\n    trg = row[\"Spanish\"]\n    \n    \n    prediction = translate_sentence(model, src, vocab_eng, vocab_esp, device)\n    \n  \n    hypotheses.append(prediction)\n    references.append([trg.lower().split()]) \n    references_for_meteor.append(trg.lower()) \n\n\nprint(\"\\nThe translations are complete, now the scores are being calculated...\")\n\n# 1. BLEU Score\nhyp_tokens = [h.split() for h in hypotheses]\nbleu_score = corpus_bleu(references, hyp_tokens, smoothing_function=SmoothingFunction().method1)\n\n# 2. METEOR Score\nmeteor_scores = []\nfor h, r in zip(hypotheses, references_for_meteor):\n    meteor_scores.append(meteor_score([r.split()], h.split()))\navg_meteor = sum(meteor_scores) / len(meteor_scores)\n\n\nprint(\"BERTScore \")\ntry:\n     \n    P, R, F1 = bert_score(hypotheses, references_for_meteor, lang=\"es\", verbose=True, batch_size=64)\n    bert_f1 = F1.mean().item()\nexcept Exception as e:\n    print(f\"bertscore error {e}\")\n    bert_f1 = 0.0\n\n\nprint(\"\\n\" + \"#\"*50)\nprint(f\" (FULL DATA TEST)\")\nprint(\"#\"*50)\nprint(f\"| Metric           | Score    |\")\nprint(f\"|------------------|----------|------------------\")\nprint(f\"| BLEU Score       | {bleu_score:.4f}   \")\nprint(f\"| METEOR Score     | {avg_meteor:.4f}   \")\nprint(f\"| BERTScore (F1)   | {bert_f1:.4f}   \")\nprint(\"#\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport numpy as np\n\n# 1. VISUALIZATION FUNCTION\ndef plot_attention_map(model, src_vocab, tgt_vocab, device, test_df):\n    model.eval()\n    \n    # Select a random sentence\n    random_row = test_df.sample(1).iloc[0]\n    src_text = random_row[\"English\"]\n    ref_text = random_row[\"Spanish\"]\n    \n    print(f\"Selected Sentence (English): {src_text}\")\n    print(f\"True Translation (Spanish): {ref_text}\")\n    \n    # --- A. Prepare Sentence ---\n    # Tokenize\n    src_tokens = [token.text.lower() for token in spacy_eng.tokenizer(src_text)]\n    src_indices = [1] + [src_vocab.stoi.get(t, 3) for t in src_tokens] + [2]\n    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n    \n    # --- B. Make Model Prediction (Word by Word) ---\n    # Here we run translate_sentence logic manually to capture vectors\n    \n    # 1. Run Encoder (Create Memory)\n    with torch.no_grad():\n        # Encoder input embedding\n        src_emb = model.src_embedding(src_tensor.transpose(0,1)) * np.sqrt(model.d_model)\n        src_emb = model.pos_encoder(src_emb)\n        # Encoder output (Memory)\n        memory = model.transformer.encoder(src_emb)\n    \n    # 2. Generate Prediction with Decoder\n    tgt_indices = [1] # <sos>\n    decoded_words = []\n    \n    for i in range(50): # Max length\n        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            # Decoder input embedding\n            tgt_emb = model.tgt_embedding(tgt_tensor.transpose(0,1)) * np.sqrt(model.d_model)\n            tgt_emb = model.pos_encoder(tgt_emb)\n            \n            # Masks\n            tgt_mask = model.generate_square_subsequent_mask(tgt_tensor.size(1)).to(device)\n            \n            # Decoder output\n            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n            \n            # Predict last word\n            output_flat = model.fc_out(output.transpose(0, 1))\n            best_guess = output_flat.argmax(2)[:, -1].item()\n            \n            if best_guess == 2: # <eos>\n                break\n            \n            tgt_indices.append(best_guess)\n            decoded_words.append(tgt_vocab.itos[best_guess])\n            \n    print(f\"Model Prediction: {' '.join(decoded_words)}\")\n\n    # --- C. CALCULATE ALIGNMENT MATRIX (Attention-like) ---\n    # Memory (Encoder Output): [Src_Len, 1, Hidden_Dim]\n    # Output (Decoder Output): [Tgt_Len, 1, Hidden_Dim]\n    \n    # Fix dimensions: [Src_Len, Hidden] and [Tgt_Len, Hidden]\n    mem_squeeze = memory.squeeze(1).cpu() \n    out_squeeze = output.squeeze(1).cpu()\n    \n    # Calculate similarity with Dot Product\n    # This process shows which Spanish word focuses on which English word.\n    attention_matrix = torch.matmul(out_squeeze, mem_squeeze.t())\n    \n    # Apply Softmax (Convert to probability)\n    attention_matrix = torch.nn.functional.softmax(attention_matrix, dim=1)\n\n    # --- D. PLOT GRAPH ---\n    src_labels = [\"<sos>\"] + src_tokens + [\"<eos>\"]\n    tgt_labels = [\"<sos>\"] + decoded_words # Predicted words\n    \n    # Fit matrix dimensions to labels (Sometimes offset due to <eos>, clipping)\n    viz_matrix = attention_matrix[:len(tgt_labels), :len(src_labels)]\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(viz_matrix.numpy(), \n                xticklabels=src_labels, \n                yticklabels=tgt_labels, \n                cmap=\"Blues\", # Blue tones like your friend sent\n                annot=False, # Don't write numbers, just color\n                linewidths=.5)\n    \n    plt.xlabel('Source (English)')\n    plt.ylabel('Prediction (Spanish)')\n    plt.title('Attention / Alignment Heatmap (Where is the Model Looking?)')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.show()\n\n# RUN AND SEE!\nplot_attention_map(model, vocab_eng, vocab_esp, device, test_df)\n\n# ==========================================\n\ndef show_examples(model, df, num_examples=10):\n    print(\"=\"*60)\n    print(f\"TRANSLATION EXAMPLES ({num_examples} Random Samples)\")\n    print(\"=\"*60)\n    \n    # From the part we reserved for testing\n    # Let's randomly select from the last 1000 rows so it's unseen during training\n    \n    # Select random indices from the end of the dataset\n    start_idx = max(0, len(df) - 1000)\n    random_indices = random.sample(range(start_idx, len(df)), num_examples)\n    \n    for i, idx in enumerate(random_indices):\n        src = df.iloc[idx][\"English\"]\n        trg = df.iloc[idx][\"Spanish\"]\n        \n        # Model prediction\n        prediction = translate_sentence(model, src, vocab_eng, vocab_esp, device)\n        \n        print(f\"({i+1})\")\n        print(f\"Source:     {src}\")\n        print(f\"Reference:  {trg}\")\n        print(f\"Prediction: {prediction}\")\n        print(\"-\" * 60)\n\n# Run Function\nshow_examples(model, df, num_examples=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}